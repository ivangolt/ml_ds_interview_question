{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Деплой ML моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Какие варианты инференса ml моделей ты знаешь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Под выводом понимается процесс использования обученной модели машинного обучения (ML) для принятия прогнозов или решений на основе новых входных данных. В зависимости от конкретного случая использования существует несколько вариантов вывода модели ML, каждый из которых оптимизирован для различных сценариев. Вот основные типы:\n",
    "\n",
    " 1. **Batch inference (пакетный вывод)**:\n",
    "   - **Определение**: Пакетный вывод подразумевает выполнение вывода на большом наборе данных одновременно, а не прогнозирование по одному.\n",
    "   - Пример использования**: Применяется для автономной обработки, когда прогнозы не должны выполняться в режиме реального времени, например, при генерации прогнозов на ночь или при массовой обработке данных для составления отчетов.\n",
    "   - **Преимущества**: \n",
    "     - Эффективность при обработке больших объемов данных.\n",
    "     - Можно оптимизировать пропускную способность, снизив накладные расходы на загрузку и инициализацию модели.\n",
    "   - **Недостатки**:\n",
    "     - Не подходит для систем реального времени.\n",
    "   - **Пример**: Прогнозирование оттока всех клиентов в конце каждого дня.\n",
    "\n",
    " 2. **Real-time inference**.\n",
    "   - **Определение**: Выводы в реальном времени обеспечивают предсказания сразу после получения новых данных. Модель должна реагировать быстро, обычно с низкой задержкой.\n",
    "   - Пример использования**: Идеально подходит для приложений, в которых решения должны приниматься мгновенно, например для обнаружения мошенничества, рекомендательных систем или чат-ботов.\n",
    "   - **Преимущества**:\n",
    "     - Мгновенная реакция на новые данные.\n",
    "   - **Недостатки**:\n",
    "     - Может потребовать оптимизации для достижения низкой задержки.\n",
    "     - При одновременном поступлении большого количества запросов может возникнуть проблема масштабируемости.\n",
    "   - Пример**: Система рекомендаций, предоставляющая персонализированные рекомендации по товарам, которые пользователи просматривают на сайте электронной коммерции.\n",
    "\n",
    " 3. **Streaming inference**\n",
    "   - **Определение**: Потоковое умозаключение непрерывно обрабатывает и делает прогнозы на основе потоков данных в реальном времени по мере их поступления.\n",
    "   - **Пример использования**: Подходит для чувствительных ко времени приложений, в которые поступает постоянный поток данных, таких как IoT-устройства, мониторинг в реальном времени или обнаружение аномалий в данных датчиков в реальном времени.\n",
    "   - **Преимущества**:\n",
    "     - Непрерывный вывод на лету.\n",
    "   - **Недостатки**:\n",
    "     - Сложность реализации и масштабирования для высокопроизводительных потоков данных.\n",
    "   - **Пример**: Мониторинг цен на акции для выявления аномального поведения рынка в реальном времени.\n",
    "\n",
    " 4. **On-Device or Edge Inference**\n",
    "   - **Определение**: В этом сценарии модель развертывается непосредственно на  устройствах (например, мобильных телефонах, устройствах IoT), а выводы делаются локально.\n",
    "   - Пример использования**: Используется, когда задержка в сети или конфиденциальность вызывают озабоченность, или когда приложение требует децентрализованной обработки (например, умные камеры, автономные транспортные средства).\n",
    "   - **Преимущества**:\n",
    "     - Низкая задержка и меньшая зависимость от подключения к Интернету.\n",
    "     - Сохранение конфиденциальности пользователей за счет локальной обработки данных.\n",
    "   - **Недостатки**:\n",
    "     - Ограниченные вычислительные ресурсы устройств могут потребовать уменьшения размера и сложности модели.\n",
    "   - **Пример**: Распознавание лиц на смартфоне без отправки данных в облако.\n",
    "\n",
    " 5. **Cloud inference**\n",
    "   - **Определение**: Выводы делаются на мощных облачных серверах, что позволяет модели масштабироваться и обслуживать множество запросов от разных пользователей или устройств.\n",
    "   - Пример использования**: Подходит для централизованной обработки данных из нескольких источников, где требуется масштабируемость и гибкость.\n",
    "   - **Преимущества**:\n",
    "     - Легко масштабируется и позволяет запускать большие и сложные модели.\n",
    "   - **Недостатки**:\n",
    "     - Может создавать сетевые задержки.\n",
    "     - Проблемы с конфиденциальностью данных, если конфиденциальные данные обрабатываются в облаке.\n",
    "   - **Пример**: Запуск большой трансформаторной модели типа GPT-4 для генерации текста.\n",
    "\n",
    " 6. **Distrubuted inference**\n",
    "   - **Определение**: При распределенном выводе рабочая нагрузка распределяется между несколькими серверами или устройствами, что позволяет параллельно обрабатывать задачи вывода.\n",
    "   - Пример использования**: Необходим для очень больших моделей или огромных объемов данных, когда одна машина не может справиться с нагрузкой.\n",
    "   - **Преимущества**:\n",
    "     - Позволяет масштабировать сложные модели (например, LLM) на нескольких машинах.\n",
    "     - Эффективно обрабатывает большие объемы запросов.\n",
    "   - **Недостатки**:\n",
    "     - Требуется сложная оркестровка и связь между распределенными системами.\n",
    "   - **Пример**: Обслуживание языковой модели с несколькими миллиардами параметров путем распределения ее частей между несколькими графическими процессорами или машинами.\n",
    "\n",
    " 7. **Hybrid inference**\n",
    "   - **Определение**: Комбинирует различные типы стратегий вывода в зависимости от ситуации. Например, можно сочетать вывод в реальном времени для чувствительных к времени данных и пакетный вывод для несрочных задач.\n",
    "   - **Случай использования**: Используется в системах, где различные задачи требуют разного уровня оперативности и вычислительной нагрузки.\n",
    "   - **Преимущества**:\n",
    "     - Гибкость и оптимизация под различные нужды.\n",
    "   - **Недостатки**:\n",
    "     - Более сложная реализация и управление.\n",
    "   - **Пример**: Приложение, в котором обнаружение мошенничества в реальном времени происходит одновременно с пакетным прогнозированием поведения пользователей в маркетинговых целях.\n",
    "\n",
    "\n",
    " 8. **Бессерверный вывод**\n",
    "   - **Определение**: Использование бессерверных архитектур, таких как AWS Lambda, для вывода моделей. Этот подход динамически распределяет ресурсы по мере необходимости, увеличивая или уменьшая их в зависимости от спроса.\n",
    "   - Пример использования**: Идеально подходит для легких моделей со спорадической нагрузкой, где важны масштабируемость и экономичность.\n",
    "   - **Преимущества**:\n",
    "     - Автоматическое масштабирование в зависимости от спроса.\n",
    "     - Нет необходимости управлять инфраструктурой.\n",
    "   - **Недостатки**:\n",
    "     - Проблемы с холодным стартом могут привести к задержкам при нечастых запросах.\n",
    "   - **Пример**: Запуск умозаключений на небольших задачах, таких как классификация изображений, загруженных пользователями в приложение для обмена фотографиями.\n",
    "\n",
    " 9. **Quantized inference**\n",
    "   - **Определение**: Квантование снижает точность параметров модели (например, с 32-битных плавающих чисел до 8-битных целых), чтобы сделать модель меньше и быстрее, не жертвуя при этом большой точностью.\n",
    "   - **Случай использования**: Оптимизирован для пограничных устройств или сценариев, в которых скорость вывода и эффективность использования памяти имеют решающее значение.\n",
    "   - **Преимущества**:\n",
    "     - Меньший объем памяти и более быстрые выводы.\n",
    "   - **Недостатки**:\n",
    "     - Некоторая потеря точности в зависимости от метода квантования.\n",
    "   - **Пример**: Развертывание квантованной нейронной сети на мобильном телефоне для обнаружения объектов.\n",
    "\n",
    " 10. **Мультимодальный вывод**\n",
    "   - **Определение**: Вывод с помощью моделей, которые одновременно обрабатывают несколько типов входных данных (например, текст, изображения, звук).\n",
    "   - Пример использования**: Применяется в сложных приложениях, где необходимо обрабатывать входные данные, поступающие от нескольких модальностей, например, для понимания видео или голосовых помощников.\n",
    "   - **Преимущества**:\n",
    "     - Работает с большим количеством входных данных.\n",
    "   - Недостатки**:\n",
    "     - Высокая вычислительная сложность и требовательность к ресурсам.\n",
    "   - **Пример**: ИИ-ассистент, обрабатывающий как голосовые команды, так и визуальный ввод с камеры.\n",
    "\n",
    "Эти варианты позволяют оптимизировать вывод модели в зависимости от таких ограничений, как задержка, масштаб, аппаратное обеспечение и сложность сценария использования.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такок feature store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Store** — это специализированное хранилище для управления признаками (features) машинного обучения, которое автоматизирует процесс создания, хранения, повторного использования и доставки признаков в модели ML. Это ключевой компонент в MLOps, который решает задачу управления признаками на всех этапах жизненного цикла ML модели.\n",
    "\n",
    "**Основные задачи Feature Store:**\n",
    "1. **Хранение признаков**:\n",
    "   - Feature Store позволяет хранить как статические, так и динамические признаки, чтобы они могли быть использованы повторно для разных моделей и экспериментов. Это помогает избежать необходимости создавать одни и те же признаки несколько раз для разных проектов.\n",
    "\n",
    "2. **Повторное использование признаков**:\n",
    "   - Повторное использование признаков, уже рассчитанных для одной модели, значительно ускоряет разработку новых моделей, снижает вычислительные затраты и уменьшает риск ошибок.\n",
    "\n",
    "3. **Обеспечение консистентности признаков**:\n",
    "   - Feature Store гарантирует, что признаки, используемые для обучения моделей, и признаки, подаваемые на модели в продакшн (в реальном времени), остаются согласованными. Это помогает предотвратить такие проблемы, как \"training-serving skew\" (расхождение признаков между обучением и продакшеном).\n",
    "\n",
    "4. **Управление версионированием признаков**:\n",
    "   - Как и модели, признаки также могут изменяться со временем. Feature Store помогает отслеживать версии признаков, обеспечивая воспроизводимость экспериментов и возможность вернуться к старым версиям признаков при необходимости.\n",
    "\n",
    "5. **Поддержка работы в реальном времени**:\n",
    "   - Feature Store может быть интегрирован с потоковыми системами для обновления признаков в реальном времени, что особенно полезно для задач, связанных с онлайн-предсказаниями (например, рекомендации, прогнозы).\n",
    "\n",
    "**Для чего используется Feature Store:**\n",
    "1. **Повышение эффективности разработки**:\n",
    "   - Позволяет инженерам и аналитикам быстро находить, переиспользовать и комбинировать признаки для разных моделей, что ускоряет разработку новых решений и улучшает их качество.\n",
    "\n",
    "2. **Снижение дублирования работы**:\n",
    "   - Платформа централизует вычисление признаков, что исключает необходимость в каждом проекте заново рассчитывать одни и те же признаки.\n",
    "\n",
    "3. **Обеспечение согласованности между обучением и продакшн**:\n",
    "   - Feature Store позволяет избежать ситуаций, когда признаки для обучения моделей отличаются от тех, что поступают на вход модели в реальном времени.\n",
    "\n",
    "4. **Оптимизация вычислительных ресурсов**:\n",
    "   - Хранение вычисленных признаков снижает необходимость повторных затрат на их пересчёт, что экономит ресурсы.\n",
    "\n",
    "**В каких случаях используется Feature Store:**\n",
    "1. **Масштабные проекты с большим количеством моделей**:\n",
    "   - В компаниях, где разрабатывается множество моделей, и есть необходимость в стандартизации признаков, Feature Store упрощает управление и ускоряет разработку.\n",
    "\n",
    "2. **Онлайн предсказания**:\n",
    "   - Если модели работают в реальном времени и требуется быстрое обновление признаков, Feature Store обеспечивает низкую задержку при подаче новых данных в модели.\n",
    "\n",
    "3. **Автоматизация и мониторинг признаков**:\n",
    "   - Feature Store полезен, когда необходимо отслеживать изменения в признаках и их влияние на модели (например, в случае \"drift\" признаков).\n",
    "\n",
    "4. **Сложные проекты с большим количеством источников данных**:\n",
    "   - Когда признаки создаются из множества различных источников (базы данных, стриминговые системы), Feature Store помогает эффективно управлять этими признаками и поддерживать их в актуальном состоянии.\n",
    "\n",
    "**Примеры систем Feature Store:**\n",
    "- **Feast** — Open-source Feature Store, разработанный совместно Gojek и Google Cloud.\n",
    "- **Hopsworks** — платформа для ML, включающая в себя Feature Store.\n",
    "- **Databricks Feature Store** — интеграция с экосистемой Databricks для управления признаками.\n",
    "\n",
    "Feature Store становится неотъемлемым инструментом в контексте MLOps, так как упрощает работу с признаками, снижает ошибки и ускоряет развитие ML моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Какие инструменты и платформы вы использовали для управления жизненным циклом моделей (например, MLflow, Kubeflow, Airflow)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для управления жизненным циклом моделей я использовал несколько инструментов, которые помогают автоматизировать и отслеживать ключевые этапы ML проектов:\n",
    "\n",
    "- MLflow: Использовал для экспериментов с моделями, отслеживания гиперпараметров, метрик и версионирования моделей. Особенно полезен для организации проектов, где требуется воспроизводимость результатов и интеграция с различными фреймворками (например, PyTorch, TensorFlow).\n",
    "- Kubeflow: Использовал для масштабирования и управления ML-пайплайнами. Kubeflow облегчает развертывание ML моделей в Kubernetes, предоставляет возможность автоматизации как процесса обучения, так и предсказания в продакшене.\n",
    "- Airflow: Применял для автоматизации рабочих процессов (workflow orchestration) и построения сложных пайплайнов для обработки данных и подготовки признаков. Airflow помогает запускать задачи по расписанию и отслеживать статус выполнения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как вы применяли Docker и Kubernetes в проектах с ML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Docker и Kubernetes** — ключевые технологии для обеспечения гибкости и масштабируемости ML решений:\n",
    "\n",
    "Docker: Использовал для создания контейнеров с моделями, библиотеками и зависимостями, что обеспечивало воспроизводимость окружения и легкость развёртывания. Это позволяет минимизировать различия в средах разработки, тестирования и продакшена. Например, модель может быть запакована в Docker-образ и развёрнута на любом сервере с Docker, независимо от специфики локальной машины.\n",
    "\n",
    "Kubernetes: Применял для масштабирования ML приложений и управления контейнерами с моделями. Kubernetes помогает автоматически управлять нагрузкой и обеспечивать высокую доступность. С его помощью можно легко разворачивать модели в продакшн и управлять их обновлениями через канареечные развертывания или A/B тестирование.\n",
    "\n",
    "Пример применения: создание сервисов для инференса модели через FastAPI или Triton Inference Server, где каждый сервис развёрнут в отдельном контейнере и управляется через Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как вы реализовывали автоматизацию процессов обучения и развёртывания моделей?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоматизация процессов обучения и развертывания моделей — важный аспект MLOps, который обеспечивает быстрое и надежное внедрение моделей в продакшн, минимизируя ручные процессы и ошибки. Я применял следующие ключевые шаги и технологии для автоматизации этих процессов.\n",
    "\n",
    "1. **CI/CD для моделей машинного обучения**\n",
    "Я использовал подход CI/CD (Continuous Integration / Continuous Deployment) для автоматизации разработки и развертывания моделей. Это включало следующие этапы:\n",
    "\n",
    "- **Continuous Integration (CI)**:\n",
    "  - **Тестирование моделей**: Для каждой новой модели или изменения существующей я настраивал автоматические тесты. Это могли быть как юнит-тесты для проверки корректности кода, так и автоматическое сравнение метрик моделей с целевыми показателями (accuracy, precision, recall). \n",
    "  - **Проверка данных**: Перед обучением моделей проверялись данные на консистентность и корректность (например, наличие пропусков, неправильные значения).\n",
    "  - **Версионирование моделей и данных**: Использовал инструменты, такие как Git для версионирования кода и **DVC** (Data Version Control) для версионирования данных, что помогало отслеживать изменения и воспроизводить эксперименты.\n",
    "\n",
    "- **Continuous Deployment (CD)**:\n",
    "  - **Обучение и валидация**: После успешного прохождения всех тестов пайплайн автоматически инициировал процесс обучения модели с использованием актуальных данных. Модели тренируются, и их качество оценивается по метрикам. Если модель удовлетворяет заданным критериям, процесс развертывания продолжается.\n",
    "  - **Развертывание в продакшн**: Модель упаковывалась в Docker-контейнер, и процесс деплоя автоматически запускался на продакшн окружении через **Kubernetes** или другие оркестраторы контейнеров. Я использовал **Helm-чарты** или **yaml-файлы** для настройки параметров деплоя.\n",
    "\n",
    "2. **Оркестрация рабочих процессов с помощью Airflow или Kubeflow Pipelines**\n",
    "Для автоматизации комплексных ML-пайплайнов, состоящих из нескольких шагов (сбор данных, предобработка, обучение, тестирование и деплой), я применял **Apache Airflow** и **Kubeflow Pipelines**:\n",
    "\n",
    "- **Apache Airflow**:\n",
    "  - Airflow позволял мне строить DAG (Directed Acyclic Graph), который автоматически запускал задачи по расписанию или при наступлении триггерного события. В пайплайны входили такие задачи, как предобработка данных, их проверка, обучение модели, кросс-валидация и деплой.\n",
    "  - Пример: Airflow DAG мог автоматически запускать обучение новой модели на еженедельной основе, обновлять модель в продакшн после тестирования, и отправлять уведомления о состоянии пайплайна через Slack или email.\n",
    "\n",
    "- **Kubeflow Pipelines**:\n",
    "  - Kubeflow был полезен для управления ML-процессами в Kubernetes среде. Я создавал пайплайны для автоматизации процессов, которые можно было масштабировать и разворачивать в контейнерах. Это позволяло запускать распределенное обучение моделей, а также разворачивать сложные пайплайны, где шаги выполнения могли зависеть друг от друга.\n",
    "  - Пример: Запуск Kubeflow Pipeline для автоматической предобработки данных, обучения модели на новых данных и их валидации. Если результат соответствовал требованиям, модель автоматически деплоилась на продакшен.\n",
    "\n",
    "3. **Использование Docker и Kubernetes для развертывания моделей**\n",
    "Docker и Kubernetes — основные технологии для обеспечения автоматизации и масштабируемости моделей:\n",
    "\n",
    "- **Docker**: Я использовал Docker для контейнеризации приложений, обеспечивая единообразие окружения на всех этапах — от разработки до продакшена. Модель вместе со всеми зависимостями упаковывалась в контейнер, что давало возможность легко переносить и запускать её на любом сервере или облачной инфраструктуре.\n",
    "  - Пример: FastAPI сервис с моделью машинного обучения упаковывался в Docker-образ, который автоматически деплоился в Kubernetes для обработки запросов.\n",
    "\n",
    "- **Kubernetes**: Kubernetes предоставлял возможность автоматизации масштабирования и управления моделями в продакшене. Я использовал **Helm-чарты** для автоматизации конфигурации развертывания, а также использовал **Kubernetes Operators** для управления процессами обновления моделей.\n",
    "  - Пример: Развертывание модели на продакшен через Kubernetes с автоматическим масштабированием в зависимости от нагрузки. Включал также мониторинг состояния модели и алерты на основе метрик производительности.\n",
    "\n",
    "4. **Мониторинг и алертинг**\n",
    "Мониторинг и контроль качества моделей после развертывания в продакшен — важная часть автоматизации процессов. Я использовал:\n",
    "\n",
    "- **Prometheus** для сбора метрик производительности модели (время отклика, количество запросов, ошибки) и состояния системы.\n",
    "- **Grafana** для визуализации этих метрик и создания дашбордов, которые отображали состояние моделей в реальном времени.\n",
    "- **Алгоритмы детектирования деградации модели (model drift)**: Настраивал метрики, которые помогали отслеживать деградацию качества предсказаний (например, изменение распределения входных данных). Если выявлялась деградация, запускался пайплайн для переобучения модели.\n",
    "\n",
    "### 5. **Автоматизация переобучения (retraining)**\n",
    "Часто возникает необходимость в переобучении моделей по мере накопления новых данных или при ухудшении качества модели. Я реализовывал автоматизацию переобучения следующим образом:\n",
    "\n",
    "- **Плановое переобучение**: С помощью Airflow или Kubeflow я запускал регулярные задачи для переобучения моделей на новых данных (например, раз в неделю или месяц). Если новые данные были доступны, они автоматически подгружались в пайплайн, и модель обучалась с нуля или на основе существующей версии.\n",
    "  \n",
    "- **Реактивное переобучение**: В случае обнаружения деградации модели по метрикам (например, с помощью мониторинга в Prometheus и Grafana), запускался процесс переобучения модели. Таким образом, модель всегда оставалась актуальной и соответствовала новым условиям.\n",
    "\n",
    "6. **Версионирование моделей и данных**\n",
    "Версионирование было неотъемлемой частью процесса автоматизации. Я использовал:\n",
    "\n",
    "- **MLflow** для версионирования моделей, где каждая модель и её параметры сохранялись в системе трекинга экспериментов.\n",
    "- **DVC** для управления версиями данных и моделей, что позволяло автоматически отслеживать изменения в данных и воспроизводить тренировки на определённых версиях данных.\n",
    "\n",
    "Таким образом, автоматизация процессов обучения и развертывания моделей позволяла эффективно управлять жизненным циклом моделей, минимизировать вмешательство человека и повышать надежность и воспроизводимость процессов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое версионирование данных и моделей, и как оно реализуется?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Версионирование данных и моделей — это процесс отслеживания изменений в наборах данных и моделях машинного обучения, позволяющий воспроизводить эксперименты, контролировать их результаты, а также управлять разными версиями данных и моделей в рамках жизненного цикла ML-проекта.\n",
    "\n",
    "1. **Зачем нужно версионирование данных и моделей?**\n",
    "В машинном обучении данные и модели постоянно изменяются: данные могут обновляться, добавляться новые источники или корректироваться ошибки, а модели могут пересобираться с новыми гиперпараметрами или архитектурами. Чтобы обеспечить воспроизводимость, управлять качеством моделей и понимать, как изменения влияют на результаты, необходимо следить за версиями.\n",
    "\n",
    "Версионирование данных и моделей позволяет:\n",
    "- **Воспроизводимость**: Возможность воспроизвести результаты эксперимента с конкретной версией данных и модели.\n",
    "- **Трассируемость**: Можно отслеживать, какие данные использовались для обучения конкретной модели и как они повлияли на результат.\n",
    "- **Разрешение конфликтов**: При работе нескольких человек над проектом можно эффективно управлять изменениями и избежать конфликтов.\n",
    "- **Контроль качества**: Легко откатываться к предыдущим версиям данных или моделей в случае ухудшения производительности.\n",
    "\n",
    "2. **Версионирование данных**\n",
    "Версионирование данных — это процесс создания и отслеживания изменений в наборах данных. Это особенно важно, так как данные в ML играют ключевую роль в результатах, и любые изменения в них могут повлиять на обучение и предсказания модели.\n",
    "\n",
    "**Инструменты для версионирования данных:**\n",
    "- **DVC (Data Version Control)**: Это Git-подобный инструмент для работы с большими наборами данных. Он позволяет версионировать данные, отслеживать изменения и восстанавливать конкретные версии данных, как это делается с кодом в Git. Примеры его использования:\n",
    "  - Хранение метаданных о данных в репозитории Git, а сами данные могут храниться в облаке или на локальных хранилищах.\n",
    "  - Легко переключаться между версиями данных для воспроизведения результатов экспериментов.\n",
    "  - Отслеживание изменений в данных с использованием коммитов, меток и веток.\n",
    "\n",
    "- **Delta Lake**: Расширение для Apache Spark, которое предоставляет поддержку транзакций и управления версиями данных в хранилище данных (data lake). Преимущество Delta Lake заключается в том, что оно позволяет управлять изменениями в больших объемах данных, обеспечивая атомарность операций.\n",
    "\n",
    " **Как это работает:**\n",
    "- Каждая версия данных сохраняется как отдельный \"snapshot\" или хэш.\n",
    "- Изменения данных (например, добавление, удаление строк или изменение признаков) фиксируются как новые версии.\n",
    "- В любой момент можно вернуться к предыдущей версии данных или переключиться на другую ветку данных, если данные менялись параллельно в разных ветках разработки.\n",
    "\n",
    "3. **Версионирование моделей**\n",
    "Версионирование моделей — это процесс отслеживания изменений в ML моделях, их архитектурах, гиперпараметрах и результатах обучения. Это позволяет управлять различными версиями моделей, тестировать их на одних и тех же данных, и выбирать лучшую модель для деплоя.\n",
    "\n",
    "**Инструменты для версионирования моделей:**\n",
    "- **MLflow**: Платформа для управления жизненным циклом моделей, которая поддерживает:\n",
    "  - Трекинг экспериментов: Включает в себя отслеживание параметров, метрик и артефактов (например, файлов с моделями).\n",
    "  - Версионирование моделей: Можно сохранять и управлять различными версиями моделей, сравнивать их производительность и метрики.\n",
    "  - Управление моделями: Модели могут сохраняться в формате, совместимом с различными фреймворками (например, TensorFlow, PyTorch), и затем деплоиться в продакшн.\n",
    "\n",
    "- **Git-LFS (Large File Storage)**: Может использоваться для версионирования больших файлов, включая модели. Хотя Git сам по себе плохо подходит для версионирования моделей (из-за их больших размеров), Git-LFS позволяет хранить большие бинарные файлы и управлять их версиями.\n",
    "\n",
    "- **Weights & Biases (W&B)**: Платформа для отслеживания экспериментов и версионирования моделей. Она интегрируется с различными ML фреймворками и позволяет сохранять и отслеживать версии моделей, экспериментов и гиперпараметров.\n",
    "\n",
    "**Как это работает:**\n",
    "- Каждая версия модели фиксируется вместе с её гиперпараметрами, метриками качества и версией данных, на которых она была обучена.\n",
    "- Версионирование помогает сравнивать различные модели по результатам (например, модель 1 обучалась на данных версии 2, а модель 2 на версии 3) и выбрать лучшую для развёртывания.\n",
    "- Можно интегрировать процесс версионирования в CI/CD пайплайн, что позволяет автоматически обновлять версии моделей при каждом новом коммите или изменении данных.\n",
    "\n",
    "4. **Как реализуется версионирование данных и моделей на практике?**\n",
    "\n",
    "**Пример с использованием DVC и MLflow:**\n",
    "1. **Подготовка данных**:\n",
    "   - Используем DVC для версионирования данных: добавляем набор данных в DVC через команду `dvc add`, что создает контрольные суммы (хэши) для каждой версии данных.\n",
    "   - Изменения данных отслеживаются и фиксируются в Git, но сами данные могут храниться в облаке или на сервере.\n",
    "\n",
    "2. **Обучение модели**:\n",
    "   - Для каждой новой версии данных, модель может быть обучена с разными параметрами.\n",
    "   - Используем MLflow для трекинга экспериментов: записываем параметры модели, метрики (например, accuracy, precision), и сохраняем саму модель.\n",
    "\n",
    "3. **Версионирование модели**:\n",
    "   - Модель сохраняется в MLflow как артефакт с метаданными (версией данных, параметрами, результатами).\n",
    "   - Можно сравнивать разные модели, отслеживать их историю и выбирать, какую модель деплоить в продакшн.\n",
    "\n",
    "4. **Развертывание и управление версиями моделей**:\n",
    "   - Разворачиваем модель с помощью CI/CD пайплайна, и если производительность новой модели не соответствует ожиданиям, можно быстро откатиться на предыдущую версию.\n",
    "\n",
    "5. **Преимущества версионирования данных и моделей**:\n",
    "- **Контроль и прозрачность**: Можно видеть историю изменений данных и моделей, что упрощает выявление причин изменений в производительности.\n",
    "- **Воспроизводимость**: Можно воспроизвести эксперименты с конкретными версиями данных и моделей.\n",
    "- **Удобство работы в команде**: Несколько разработчиков могут параллельно работать с разными версиями данных и моделей, минимизируя конфликты.\n",
    "- **Безопасность**: В случае отката на предыдущие версии можно быстро восстановить предыдущие данные или модель.\n",
    "\n",
    "6. **Заключение**\n",
    "Версионирование данных и моделей — это основа воспроизводимости и прозрачности в проектах машинного обучения. Используя такие инструменты, как DVC, MLflow и Delta Lake, можно эффективно управлять версиями данных и моделей, что способствует более организованному и надежному процессу разработки и внедрения ML решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
