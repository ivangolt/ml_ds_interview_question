{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Объясни как работает Vision Transformer\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformer (ViT) — это архитектура для решения задач компьютерного зрения, основанная на механизме трансформеров, которые широко используются в задачах обработки текста. В отличие от свёрточных нейронных сетей (CNN), которые обычно применяются в задачах обработки изображений, ViT использует идею представления изображения как последовательности «токенов», аналогично тому, как трансформеры обрабатывают текст.\n",
    "\n",
    " **Основные этапы работы Vision Transformer:**\n",
    "\n",
    "1. **Разбиение изображения на патчи**:\n",
    "   Изображение разделяется на небольшие фрагменты (патчи). Например, если исходное изображение имеет размер \\(224 * 224), его можно разбить на патчи размером (16 * 16), что приведёт к 196 патчам ведь (224/16 = 14), и (14 * 14 = 196).\n",
    "\n",
    "2. **Линеаризация патчей**:\n",
    "   Каждый патч разворачивается в одномерный вектор. Если размер одного патча (16 * 16), то после развёртки его размерность будет (16 * 16 = 256). Каждый из этих векторов можно рассматривать как токен, аналогичный токенам в текстовых моделях трансформеров.\n",
    "\n",
    "3. **Добавление позиционных эмбеддингов**:\n",
    "   Поскольку трансформеры не обладают встроенной способностью понимать порядок входных данных, к каждому патчу добавляется позиционная информация (позиционные эмбеддинги), которая помогает модели учитывать взаимное расположение патчей в исходном изображении.\n",
    "\n",
    "4. **Прохождение через энкодер трансформера**:\n",
    "   Последовательность токенов (развёрнутых патчей с позиционными эмбеддингами) поступает в энкодер трансформера. Этот энкодер состоит из слоёв самовнимания (self-attention) и полносвязных слоёв (Feedforward Neural Network, FFN).\n",
    "   - **Механизм самовнимания** позволяет каждому патчу «внимательно» отслеживать информацию в других патчах, что помогает улавливать глобальные зависимости между различными частями изображения.\n",
    "   - **Полносвязные слои** обрабатывают выходы слоёв самовнимания, добавляя нелинейности и более сложные представления.\n",
    "\n",
    "5. **Классификационная голова**:\n",
    "   После нескольких слоёв трансформера один из токенов (обычно специальный класс-токен, добавляемый в начале последовательности) поступает в классификационную голову (обычно это линейный слой), которая выдаёт предсказания для задачи (например, к какому классу относится изображение).\n",
    "\n",
    " **Преимущества ViT:**\n",
    "- **Глобальная обработка данных**: Трансформеры способны эффективно захватывать глобальные зависимости в данных, что даёт ViT преимущество перед CNN, которые работают с локальными фильтрами.\n",
    "- **Простота архитектуры**: Модель не использует сложные свёрточные операции, а это упрощает архитектуру.\n",
    "- **Эффективность при наличии большого объёма данных**: ViT особенно эффективен на больших датасетах, таких как ImageNet, и часто превосходит CNN, когда данных достаточно.\n",
    "\n",
    "**Недостатки:**\n",
    "- **Необходимость в больших данных**: ViT требует больше данных для обучения по сравнению с CNN, чтобы эффективно обучать механизм самовнимания и выявлять глобальные зависимости.\n",
    "- **Больше вычислительных ресурсов**: Механизм самовнимания в трансформерах требует больше вычислительных ресурсов для обработки, особенно на больших изображениях.\n",
    "\n",
    "ViT показал, что трансформеры могут быть не менее эффективны в задачах компьютерного зрения, чем свёрточные сети, если правильно адаптировать их под специфику визуальных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
