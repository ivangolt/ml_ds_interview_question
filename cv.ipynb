{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'asttokens'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Объясните архитектуру CNN и как она работает?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свёрточные нейронные сети (Convolutional Neural Networks, CNN) представляют собой архитектуры глубокого обучения, разработанные специально для работы с изображениями и обработки данных с пространственной структурой. Их структура состоит из нескольких типов слоев, которые позволяют выявлять сложные паттерны в изображениях. Вот основные компоненты архитектуры CNN и их работа:\n",
    "\n",
    " **Основные компоненты архитектуры CNN**\n",
    "1. **Слои свёртки (Convolutional Layers)**:\n",
    "   - **Фильтры (ядра)**: В этом слое применяются фильтры (ядра свёртки) для изображения. Каждый фильтр – это небольшая матрица, которая «скользит» по изображению и выполняет операцию свёртки, то есть вычисляет значение для каждой части изображения.\n",
    "   - **Выходные карты признаков**: Каждый фильтр выделяет специфический признак, например, грани, текстуры или формы, и формирует выходную карту признаков (feature map), отражающую наличие этих признаков на изображении.\n",
    "   - **Параметры фильтра**: Параметры фильтров обучаются в процессе обучения модели, что позволяет CNN автоматически выделять информативные признаки.\n",
    "\n",
    "2. **Слои активации (Activation Layers)**:\n",
    "   - Обычно после слоя свёртки применяется нелинейная функция активации, например, ReLU (Rectified Linear Unit), которая оставляет только положительные значения, делая сеть нелинейной и увеличивая её выразительность. Это позволяет модели выделять более сложные и нелинейные зависимости.\n",
    "\n",
    "3. **Слои пуллинга (Pooling Layers)**:\n",
    "   - **Max Pooling или Average Pooling**: Эти слои уменьшают размер карты признаков, беря максимум или среднее значение в каждом участке карты. Уменьшение размерности помогает снизить вычислительные затраты и сделать модель более устойчивой к смещениям и искажениям объекта.\n",
    "   - **Цель**: Пуллинг уменьшает количество параметров и вычислительные затраты, а также снижает вероятность переобучения.\n",
    "\n",
    "4. **Полносвязные слои (Fully Connected Layers)**:\n",
    "   - В конце свёрточных и пуллинговых слоев выходные карты признаков выравниваются (flattening), и полученные данные передаются в несколько полносвязных слоев.\n",
    "   - Эти слои применяются для классификации выделенных признаков. Полносвязные слои обрабатывают комбинации признаков и позволяют выделить зависимости между ними, необходимые для точной классификации.\n",
    "\n",
    "**Как работает CNN (на примере задачи классификации изображений)**\n",
    "1. **Свёртка и активация**: На входное изображение накладываются фильтры, выделяющие простые признаки (например, края). Далее применяются функции активации, что позволяет увеличить сложность модели и способность выделять нелинейные паттерны.\n",
    "  \n",
    "2. **Пуллинг**: После каждого слоя свёртки и активации используется слой пуллинга, чтобы уменьшить размер карты признаков и оставить только наиболее значимые признаки.\n",
    "\n",
    "3. **Углубление признаков**: По мере увеличения количества слоев свёртки и пуллинга, сеть учится более сложным и высокоуровневым признакам, таким как формы и текстуры.\n",
    "\n",
    "4. **Полносвязные слои и классификация**: После выделения признаков с помощью свёрток и пуллинга данные передаются в полносвязные слои, где сеть анализирует комбинации признаков и классифицирует изображение на основе этих данных.\n",
    "\n",
    "**Примерная архитектура CNN (для задач классификации)**\n",
    "   ```\n",
    "   Входное изображение -> [Свёрточный слой + ReLU] -> [Пуллинг] -> [Свёрточный слой + ReLU] -> [Пуллинг] -> [Полносвязные слои] -> [Выходной слой (классификация)]\n",
    "   ```\n",
    "\n",
    "**Преимущества CNN**\n",
    "- **Выделение признаков**: CNN автоматически учатся выделять значимые признаки из изображения.\n",
    "- **Пространственная инвариантность**: Благодаря свёрткам и пуллингу, CNN более устойчивы к смещениям и деформациям объектов.\n",
    "- **Эффективность**: Меньшее количество параметров по сравнению с полносвязными сетями позволяет CNN обрабатывать изображения быстрее и с меньшим объёмом памяти.\n",
    "\n",
    "**Заключение**\n",
    "Свёрточные нейронные сети стали основой в компьютерном зрении благодаря своей способности выделять сложные признаки и пространственные зависимости. Они используются для задач классификации, детекции, сегментации и других, требующих понимания содержимого изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужны слои свертки и пуллинга в CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слои свёртки и пуллинга являются ключевыми элементами архитектуры свёрточных нейронных сетей (CNN), обеспечивающими выделение признаков и сокращение размера данных. Они позволяют сети эффективно работать с изображениями, извлекать нужные паттерны и повышать устойчивость модели к различным искажениям.\n",
    "\n",
    "**1. Слои свёртки (Convolutional Layers)**\n",
    "Слои свёртки используются для автоматического выделения признаков из изображения, таких как контуры, текстуры и сложные формы.\n",
    "\n",
    "- **Зачем нужны слои свёртки?**\n",
    "   - **Выделение локальных признаков**: Свёртка позволяет обнаруживать локальные паттерны (например, углы, края, текстуры) в разных частях изображения. На каждом этапе свёртки сеть учится более сложным признакам, начиная с простых (краёв) и переходя к более абстрактным.\n",
    "   - **Пространственная инвариантность**: Слои свёртки помогают модели стать устойчивой к смещениям объектов в изображении, что делает CNN более адаптивными к различным позициям объектов.\n",
    "   - **Уменьшение количества параметров**: Свёртка позволяет обрабатывать изображения более эффективно, применяя небольшое количество параметров (фильтры) к части изображения, а не к каждому пикселю. Это снижает вычислительные затраты и позволяет CNN обрабатывать данные быстрее.\n",
    "\n",
    "- **Как работают слои свёртки?**\n",
    "   - Каждый фильтр (ядро свёртки) скользит по изображению и вычисляет значения для каждого участка. Фильтр обновляется в процессе обучения, и CNN адаптируется к выделению значимых признаков на основе данных.\n",
    "\n",
    "**2. Слои пуллинга (Pooling Layers)**\n",
    "Слои пуллинга используются для уменьшения размерности карты признаков, сохраняя при этом самые значимые элементы. Основные типы пуллинга — это Max Pooling (максимум в каждой области) и Average Pooling (среднее в каждой области).\n",
    "\n",
    "- **Зачем нужны слои пуллинга?**\n",
    "   - **Снижение размерности**: Пуллинг уменьшает разрешение карты признаков, что уменьшает количество вычислений в последующих слоях и помогает избежать переобучения.\n",
    "   - **Устойчивость к изменениям**: Пуллинг делает сеть более устойчивой к небольшим смещениям и деформациям объектов в изображении, что улучшает её способность к генерализации.\n",
    "   - **Выделение важных признаков**: Max Pooling, например, сохраняет только наиболее активные признаки в каждой области, что помогает выделить наиболее информативные части карты признаков.\n",
    "\n",
    "- **Как работают слои пуллинга?**\n",
    "   - Пуллинг разбивает карту признаков на небольшие участки и применяет к каждому участок выбранный метод (максимум или среднее), создавая таким образом уменьшенную карту признаков.\n",
    "\n",
    "**Роль слоёв свёртки и пуллинга в CNN**\n",
    "Слои свёртки и пуллинга работают вместе, чтобы выделять и сжимать значимые признаки, позволяя CNN эффективно классифицировать или анализировать изображения. В результате CNN могут автоматически учиться сложным пространственным структурам данных, оставаясь при этом компактными и эффективными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как решается проблема переобучения в CNN? Какие существуют техники регуляризации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения проблемы переобучения в CNN (как и в других нейронных сетях) применяются различные методы регуляризации, которые помогают модели обобщаться лучше на новых данных. Вот основные техники:\n",
    "\n",
    "1. **Dropout**\n",
    "   - **Суть метода**: На каждом этапе обучения случайно обнуляется часть нейронов, что предотвращает сильную зависимость между ними.\n",
    "   - **Как это работает**: При обучении сети нейроны, выбранные случайным образом, отключаются (устанавливаются в ноль), что позволяет сети учиться более устойчивым и обобщающим признакам, а не \"запоминать\" данные.\n",
    "   - **Типичный параметр**: Вероятность dropout обычно составляет от 0.2 до 0.5, что позволяет сбалансировать регуляризацию и сохранение информации.\n",
    "  \n",
    "2. **Data Augmentation (Аугментация данных)**\n",
    "   - **Суть метода**: Генерация дополнительных данных путём преобразований изображений, таких как повороты, сдвиги, обрезка, изменения яркости, зеркальные отражения и пр.\n",
    "   - **Как это работает**: За счёт аугментации увеличивается количество вариантов исходных изображений, на которых модель обучается. Это делает модель устойчивой к различным видам искажений и улучшает её способность к обобщению.\n",
    "   - **Типичные техники**: Вращение, горизонтальное и вертикальное отражение, обрезка, масштабирование, изменение цвета или яркости.\n",
    "\n",
    "3. **L2-регуляризация (также известная как регуляризация по норме весов)**\n",
    "   - **Суть метода**: Добавление штрафа за большие значения весов в функцию потерь.\n",
    "   - **Как это работает**: L2-регуляризация добавляет в функцию потерь параметр, который пропорционален квадрату величины весов (обычно 𝜆 * ||W||², где 𝜆 — коэффициент регуляризации). Это предотвращает чрезмерное увеличение весов, что способствует снижению вероятности переобучения.\n",
    "   - **Типичный параметр**: Значение 𝜆 подбирается экспериментально и часто варьируется от 0.0001 до 0.1 в зависимости от сложности задачи.\n",
    "\n",
    "4. **Batch Normalization**\n",
    "   - **Суть метода**: Нормализация активаций на каждом слое для стабилизации и ускорения обучения.\n",
    "   - **Как это работает**: Batch normalization нормализует значения внутри каждой мини-партии, что позволяет сети легче обучаться и делает её менее чувствительной к изменениям в параметрах. Это улучшает генерализацию и снижает вероятность переобучения.\n",
    "   - **Дополнительные преимущества**: Batch Normalization также позволяет использовать более высокие скорости обучения, что ускоряет обучение модели.\n",
    "\n",
    "5. **Ранняя остановка (Early Stopping)**\n",
    "   - **Суть метода**: Остановка обучения при достижении определённого критерия на валидационной выборке.\n",
    "   - **Как это работает**: Если точность на валидационных данных перестаёт улучшаться (или даже ухудшается) в течение нескольких эпох, обучение останавливается. Это предотвращает перенастройку модели на тренировочные данные, позволяя завершить обучение в оптимальный момент.\n",
    "   - **Типичный параметр**: Число эпох, в течение которых модель может не улучшать метрику на валидационных данных (например, 5 или 10).\n",
    "\n",
    "6. **Уменьшение сложности модели**\n",
    "   - **Суть метода**: Уменьшение количества параметров модели для уменьшения её сложности.\n",
    "   - **Как это работает**: Уменьшение количества слоёв или нейронов в слоях помогает избежать переобучения, особенно на малых выборках. Модель остаётся более простой и не запоминает данные, а обучается выявлять общие паттерны.\n",
    "   - **Когда применять**: Применяется на ограниченных или малых наборах данных, где сложная архитектура может вызвать сильное переобучение.\n",
    "\n",
    "7. **Transfer Learning с Fine-tuning**\n",
    "   - **Суть метода**: Использование предобученной модели с дообучением на новом наборе данных.\n",
    "   - **Как это работает**: Начальные слои замораживаются, а обучение происходит на последних слоях модели, что позволяет использовать уже полученные знания из предыдущих задач и избежать переобучения на новом, небольшом наборе данных.\n",
    "   - **Преимущества**: Особенно эффективно при обучении на специализированных данных, когда доступно малое количество данных.\n",
    "\n",
    "8. **Контроль размера мини-батча**\n",
    "   - **Суть метода**: Подбор оптимального размера мини-батча для обучения.\n",
    "   - **Как это работает**: Слишком большой мини-батч может привести к снижению качества обобщения, так как модель видит меньшее разнообразие данных на каждой итерации. Оптимальный размер мини-батча позволяет снизить риск переобучения, сохраняя разнообразие в процессе обучения.\n",
    "   - **Типичные значения**: От 16 до 128, в зависимости от размера и сложности набора данных.\n",
    "\n",
    "Эти методы помогают снизить вероятность переобучения и улучшают способность модели к генерализации, что делает её более устойчивой и эффективной на реальных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое ядро свёртки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ядро свертки, также известное как фильтр или маска, представляет собой матрицу небольшого размера, которая скользит по входным данным (например, изображению) для выполнения операции свертки. Каждый элемент ядра представляет собой вес, который умножается на соответствующий пиксель во входных данных, а затем суммируется, чтобы получить выходное значение для конкретного пикселя в выходном изображении. Ядро свертки позволяет выделять различные характеристики входных данных, такие как границы, текстуры или общие шаблоны, и эффективно сжимать информацию за счет разделения параметров между пикселями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое pooling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooling** - это операция, которая выполняется после свертки в сверточных нейронных сетях. Она используется для уменьшения размерности пространства признаков путем агрегации информации из набора соседних пикселей или признаков. Обычно в пулинге выбирается одно значение из набора соседних пикселей или признаков (например, максимальное значение в случае максимального пулинга, среднее значение в случае среднего пулинга) и используется как представление для этого набора. Это позволяет уменьшить количество параметров и вычислений в сети, улучшить инвариантность к масштабированию и улучшить обобщающую способность модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое Receptive Field?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Receptive Field (рецептивное поле)** в контексте сверточных нейронных сетей представляет собой область входных данных, которая влияет на активацию определенного нейрона в слое. Он определяет, какие пиксели или признаки во входном изображении влияют на вывод определенного нейрона. Размер рецептивного поля определяется архитектурой сети и параметрами слоев, такими как размер ядра свертки и размеры пулинга. Увеличение размера рецептивного поля позволяет модели анализировать более широкие контексты входных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Какие есть способы решения проблем оптимизации сверточных сетей?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации свёрточных нейронных сетей (CNN) существует несколько подходов, направленных на улучшение скорости и уменьшение потребляемых ресурсов без значительного ухудшения качества модели. Вот основные методы:\n",
    "\n",
    "1. **Квантование (Quantization)**  \n",
    "   Преобразование весов и активаций модели из 32-битных значений в более компактные, например, 8-битные целочисленные значения. Это уменьшает объём памяти и ускоряет выполнение, так как целочисленные операции быстрее, чем с плавающей точкой. Квантование может быть как статическим, так и динамическим.\n",
    "\n",
    "2. **Принудительная разреженность (Pruning)**  \n",
    "   Удаление наименее значимых весов из сети. Существуют различные подходы, включая **магнитудное обрезание** (удаление весов с малыми значениями) и **структурное обрезание** (удаление целых фильтров или каналов). Это снижает количество операций и объем памяти при минимальном ухудшении точности.\n",
    "\n",
    "3. **Сжатие и дистилляция знаний (Knowledge Distillation)**  \n",
    "   Использование более сложной, «учительской» модели для обучения более компактной, «студенческой» модели. Студент обучается воспроизводить ответы учителя, что позволяет передать знания от сложной модели к простой без значительной потери качества.\n",
    "\n",
    "4. **Оптимизация архитектуры**  \n",
    "   Разработка специализированных архитектур, таких как MobileNet, SqueezeNet и EfficientNet, которые используют глубинно-свёрточные или сепарабельные свёртки. Они выполняют меньше операций при сопоставимой производительности, особенно полезны для мобильных устройств и внедрения на устройствах с ограниченными ресурсами.\n",
    "\n",
    "5. **Факторизация свёрток**  \n",
    "   Разложение стандартных свёрток на более мелкие, например, разбиение свёрток \\(3 \\times 3\\) на комбинацию нескольких \\(1 \\times 3\\) и \\(3 \\times 1\\) свёрток, что уменьшает количество операций.\n",
    "\n",
    "6. **Пакетная нормализация и передискретизация данных**  \n",
    "   Пакетная нормализация (Batch Normalization) стабилизирует и ускоряет процесс обучения, а передискретизация (Pooling) позволяет уменьшить размер карты признаков. Правильная настройка этих слоев помогает снизить вычислительную нагрузку.\n",
    "\n",
    "7. **Использование компиляторов моделей (Model Compilers)**  \n",
    "   Специальные компиляторы (например, TensorRT от NVIDIA, TVM от Apache или OpenVINO от Intel) преобразуют модели в более эффективные формы для работы на определённых устройствах, используя оптимизированные под архитектуру инструкции.\n",
    "\n",
    "8. **Асинхронное и распараллеленное выполнение операций**  \n",
    "   Распределение операций по нескольким ядрам процессора или графическим процессорам. Например, параллелизм на уровне данных или моделей, позволяющий распараллелить вычисления и ускорить обучение или выполнение на больших наборах данных.\n",
    "\n",
    "Выбор методов зависит от задачи, требований к точности, доступного оборудования и целевого устройства для внедрения модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как свертка будет обрабатывать цветное изображение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сверточная нейронная сеть обрабатывает цветное изображение, представленное в виде трехмерного тензора, где каждый канал соответствует отдельному цветовому каналу (красному, зеленому и синему). Свертка применяется независимо к каждому каналу изображения. Каждый фильтр свертки также является трехмерным, имеющим ширину, высоту и количество каналов, совпадающее с количеством каналов во входном изображении. Во время операции свертки фильтр перемещается по всему изображению, применяясь к каждому пикселю и каждому каналу изображения. Результатом операции свертки будет трехмерный тензор, представляющий собой карту признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужен пулинг в свертках, почему нельзя обойтись без него или использовать просто свертку со страйдом? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пулинг (pooling)** в сверточных нейронных сетях используется для уменьшения размерности признакового пространства, что помогает улучшить вычислительную эффективность и уменьшить переобучение. Пулинг также обеспечивает инвариантность к небольшим трансляциям объектов в изображении. Хотя свертки со страйдом также могут уменьшать размерность, пулинг предоставляет дополнительные преимущества, такие как инвариантность к масштабу и позиции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Объясни как работает Vision Transformer\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformer (ViT) — это архитектура для решения задач компьютерного зрения, основанная на механизме трансформеров, которые широко используются в задачах обработки текста. В отличие от свёрточных нейронных сетей (CNN), которые обычно применяются в задачах обработки изображений, ViT использует идею представления изображения как последовательности «токенов», аналогично тому, как трансформеры обрабатывают текст.\n",
    "\n",
    " **Основные этапы работы Vision Transformer:**\n",
    "\n",
    "1. **Разбиение изображения на патчи**:\n",
    "   Изображение разделяется на небольшие фрагменты (патчи). Например, если исходное изображение имеет размер \\(224 * 224), его можно разбить на патчи размером (16 * 16), что приведёт к 196 патчам ведь (224/16 = 14), и (14 * 14 = 196).\n",
    "\n",
    "2. **Линеаризация патчей**:\n",
    "   Каждый патч разворачивается в одномерный вектор. Если размер одного патча (16 * 16), то после развёртки его размерность будет (16 * 16 = 256). Каждый из этих векторов можно рассматривать как токен, аналогичный токенам в текстовых моделях трансформеров.\n",
    "\n",
    "3. **Добавление позиционных эмбеддингов**:\n",
    "   Поскольку трансформеры не обладают встроенной способностью понимать порядок входных данных, к каждому патчу добавляется позиционная информация (позиционные эмбеддинги), которая помогает модели учитывать взаимное расположение патчей в исходном изображении.\n",
    "\n",
    "4. **Прохождение через энкодер трансформера**:\n",
    "   Последовательность токенов (развёрнутых патчей с позиционными эмбеддингами) поступает в энкодер трансформера. Этот энкодер состоит из слоёв самовнимания (self-attention) и полносвязных слоёв (Feedforward Neural Network, FFN).\n",
    "   - **Механизм самовнимания** позволяет каждому патчу «внимательно» отслеживать информацию в других патчах, что помогает улавливать глобальные зависимости между различными частями изображения.\n",
    "   - **Полносвязные слои** обрабатывают выходы слоёв самовнимания, добавляя нелинейности и более сложные представления.\n",
    "\n",
    "5. **Классификационная голова**:\n",
    "   После нескольких слоёв трансформера один из токенов (обычно специальный класс-токен, добавляемый в начале последовательности) поступает в классификационную голову (обычно это линейный слой), которая выдаёт предсказания для задачи (например, к какому классу относится изображение).\n",
    "\n",
    " **Преимущества ViT:**\n",
    "- **Глобальная обработка данных**: Трансформеры способны эффективно захватывать глобальные зависимости в данных, что даёт ViT преимущество перед CNN, которые работают с локальными фильтрами.\n",
    "- **Простота архитектуры**: Модель не использует сложные свёрточные операции, а это упрощает архитектуру.\n",
    "- **Эффективность при наличии большого объёма данных**: ViT особенно эффективен на больших датасетах, таких как ImageNet, и часто превосходит CNN, когда данных достаточно.\n",
    "\n",
    "**Недостатки:**\n",
    "- **Необходимость в больших данных**: ViT требует больше данных для обучения по сравнению с CNN, чтобы эффективно обучать механизм самовнимания и выявлять глобальные зависимости.\n",
    "- **Больше вычислительных ресурсов**: Механизм самовнимания в трансформерах требует больше вычислительных ресурсов для обработки, особенно на больших изображениях.\n",
    "\n",
    "ViT показал, что трансформеры могут быть не менее эффективны в задачах компьютерного зрения, чем свёрточные сети, если правильно адаптировать их под специфику визуальных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
