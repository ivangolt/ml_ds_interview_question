{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Structured Semantic Model** (DSSM) — это глубокая нейронная модель, разработанная для вычисления семантического сходства между двумя объектами (например, текстовыми запросами и документами). Она была предложена командой Microsoft Research и активно используется в задачах ранжирования, информационного поиска, рекомендаций и обработки естественного языка (NLP).\n",
    "\n",
    "**Основная идея DSSM**\n",
    "Цель DSSM — преобразовать два входных объекта в общее семантическое пространство, где можно измерить их сходство. Модель извлекает скрытые (латентные) признаки из входных данных и вычисляет семантическое сходство с помощью таких метрик, как косинусное сходство.\n",
    "\n",
    "---\n",
    "\n",
    "**Архитектура DSSM**\n",
    "\n",
    " 1. **Входные данные**\n",
    "DSSM может работать с разными типами данных:\n",
    "- **Текстовые данные** (например, запросы, документы).\n",
    "- **Мультимодальные данные** (например, текст и изображения).\n",
    "\n",
    "Для текстов обычно используется **bag-of-words**, **TF-IDF**, или векторизация символов как начальное представление.\n",
    "\n",
    " 2. **Этап эмбеддинга**\n",
    "Входные данные пропускаются через несколько уровней нейронной сети, чтобы преобразовать их в низкоразмерные представления в общем семантическом пространстве. \n",
    "\n",
    "**Традиционный DSSM**\n",
    "- Использует **fully connected layers** (полносвязные слои).\n",
    "- Слои кодируют латентные признаки, которые помогают улавливать семантические зависимости.\n",
    "\n",
    " **Варианты DSSM**\n",
    "- **CDSSM (Convolutional DSSM)**: Добавляет сверточные слои (CNN) для извлечения локальных признаков, особенно полезных для текстов.\n",
    "- **RNN DSSM**: Использует рекуррентные нейронные сети (RNN или LSTM), чтобы учитывать последовательность и контекст.\n",
    "\n",
    "3. **Общее семантическое пространство**\n",
    "Модель преобразует входные данные (например, запросы и документы) в два вектора одинаковой размерности. Эти вектора расположены в общем пространстве таким образом, что их сходство отражает семантическую близость исходных объектов.\n",
    "\n",
    " 4. **Функция сходства**\n",
    "Сходство между векторами обычно измеряется с помощью:\n",
    "- Косинусного сходства: \n",
    "  \\[\n",
    "  \\text{similarity}(q, d) = \\frac{q \\cdot d}{\\|q\\| \\|d\\|}\n",
    "  \\]\n",
    "- Других функций (например, евклидового расстояния).\n",
    "\n",
    " 5. **Функция потерь**\n",
    "Для обучения DSSM используется функция потерь, которая максимизирует сходство релевантных пар (запрос-документ) и минимизирует сходство нерелевантных пар. Популярный подход — **cross-entropy loss** или **hinge loss**.\n",
    "\n",
    "---\n",
    "\n",
    "**Процесс обучения**\n",
    "1. **Данные**: Для обучения требуются пары входов:\n",
    "   - Положительные пары (запросы и релевантные документы).\n",
    "   - Негативные пары (запросы и нерелевантные документы).\n",
    "2. **Оптимизация**: Путем градиентного спуска модель оптимизирует параметры, чтобы минимизировать функцию потерь.\n",
    "\n",
    "---\n",
    "\n",
    "**Преимущества DSSM**\n",
    "1. **Гибкость**: DSSM может работать с любыми типами данных, которые можно преобразовать в векторное представление.\n",
    "2. **Семантическая интерпретация**: Модель понимает глубинные значения текста, а не только поверхностное совпадение слов.\n",
    "3. **Производительность**: Глубокие нейронные сети позволяют извлекать сложные и нелинейные признаки.\n",
    "\n",
    "---\n",
    "\n",
    "**Применение DSSM**\n",
    "1. **Информационный поиск**:\n",
    "   - Оценка релевантности документов запросу.\n",
    "   - Ранжирование документов в поисковой выдаче.\n",
    "2. **Рекомендательные системы**:\n",
    "   - Сопоставление интересов пользователя с элементами.\n",
    "3. **NLP задачи**:\n",
    "   - Определение сходства текстов.\n",
    "   - Машинный перевод.\n",
    "4. **Мультимодальные задачи**:\n",
    "   - Сопоставление текста с изображениями или видео.\n",
    "\n",
    "---\n",
    "\n",
    "**Пример использования DSSM в информационном поиске**\n",
    "\n",
    "1. **Запрос пользователя** (например, \"лучшие рестораны поблизости\") преобразуется в векторное представление.\n",
    "2. **Документы в индексе** также преобразуются в векторы.\n",
    "3. Вычисляется сходство между запросом и каждым документом.\n",
    "4. Документы сортируются по убыванию сходства.\n",
    "\n",
    "---\n",
    "\n",
    "**Ограничения DSSM**\n",
    "1. **Большая вычислительная сложность**: Глубокие нейронные сети требуют значительных ресурсов для обучения и инференса.\n",
    "2. **Зависимость от данных**: Для достижения высокой точности требуется большое количество релевантных данных.\n",
    "3. **Чувствительность к качеству эмбеддингов**: Плохие эмбеддинги на входе могут ухудшить производительность.\n",
    "\n",
    "---\n",
    "\n",
    "**Эволюция DSSM**\n",
    "После появления DSSM были разработаны более продвинутые модели:\n",
    "- **BERT-based модели**: Например, модели семейства SBERT.\n",
    "- **Transformer-модели**: Учитывают контекст и могут быть более точными, чем DSSM.\n",
    "- **Neural Matching Models**: Современные подходы, учитывающие не только семантическое, но и точное совпадение слов.\n",
    "\n",
    "Тем не менее, DSSM остаётся основой для многих задач семантического поиска и рекомендаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
